{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "JyViNokqdO33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingGridSearchCV"
      ],
      "metadata": {
        "id": "70eLL_u-B35e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Colab Env"
      ],
      "metadata": {
        "id": "qdqWsgiJdLFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqFINvmddJHr",
        "outputId": "e48e202d-5aa9-4157-f1af-d23ec1a8f79e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/UNI-STUFF/SC1015/airbnb_with_landmark_metro_features.csv\")"
      ],
      "metadata": {
        "id": "dwILv9zBddI9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format and Clean Base Data"
      ],
      "metadata": {
        "id": "afQhiyAWY2I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevant columns\n",
        "columns_to_drop = [\n",
        "    'thumbnail_url', 'amenities', 'description', 'host_has_profile_pic',\n",
        "    'host_identity_verified', 'host_response_rate', 'host_since', 'name',\n",
        "    'neighbourhood', 'zipcode'\n",
        "]\n",
        "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Condense room_type\n",
        "df['room_type'] = df['room_type'].replace({\n",
        "    'Entire home/apt': \"Private\",\n",
        "    'Private room': \"Shared\",\n",
        "    'Shared room': \"Shared\"\n",
        "})\n",
        "\n",
        "# Condense property_type\n",
        "property_typ_vc = df[\"property_type\"].value_counts() / 50\n",
        "others = property_typ_vc[property_typ_vc < 5].index\n",
        "df['property_type'] = df['property_type'].replace(others, 'Other')\n",
        "\n",
        "# Ensure review dates are in datetime format\n",
        "df['first_review'] = pd.to_datetime(df['first_review'], errors='coerce')\n",
        "df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
        "\n",
        "# Create a review duration feature\n",
        "df['review_duration'] = (df['last_review'] - df['first_review']).dt.days\n",
        "\n",
        "# Prepare a new cleaned DataFrame\n",
        "clean_df = df[[\n",
        "    'property_type', 'room_type', 'accommodates', 'bathrooms', 'bed_type',\n",
        "    'cancellation_policy', 'cleaning_fee', 'city', 'instant_bookable',\n",
        "    'latitude', 'longitude', 'number_of_reviews', 'review_scores_rating',\n",
        "    'bedrooms', 'beds'\n",
        "]].copy()\n",
        "\n",
        "# Add transformed or derived features\n",
        "clean_df['price'] = np.exp(df['log_price'])  # Transform log_price back to price\n",
        "clean_df['instant_bookable'] = clean_df['instant_bookable'].map({'t': True, 'f': False})  # Map to boolean\n",
        "clean_df['total_review_score'] = clean_df['number_of_reviews'] * clean_df['review_scores_rating']  # Derived feature\n",
        "\n",
        "# Create seperate dataframe with engineered features\n",
        "temp_df = df[['landmarks_within_500m', 'landmarks_within_1000m',\n",
        "                'landmarks_within_2000m', 'avg_distance_to_landmarks',\n",
        "                'metro_within_500m', 'metro_within_1000m', 'metro_within_5000m',\n",
        "                'shortest_distance_to_metro']].copy()\n",
        "eng_df = pd.concat([clean_df, temp_df], axis=1)"
      ],
      "metadata": {
        "id": "Zd1NmubZY5DP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Define Utility Functions\n",
        "Create reusable functions for:\n",
        "- Preparing features and target variables dynamically.\n",
        "- Evaluating models and printing metrics.\n",
        "- Running hyperparameter tuning using grid search."
      ],
      "metadata": {
        "id": "9NcGkHS3rLQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define Utility Functions\n",
        "def prepare_data(df, target_column):\n",
        "    \"\"\"\n",
        "    Prepare features and target variables for a dataset.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input dataset.\n",
        "        target_column (str): The name of the target column.\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test: Train-test split datasets.\n",
        "    \"\"\"\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # One-hot encode categorical variables\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def evaluate(model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fit the model, make predictions, and compute evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The ML model to evaluate.\n",
        "        X_train, X_test, y_train, y_test: Train-test split datasets.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing R² and MSE metrics.\n",
        "    \"\"\"\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"r2\": r2_score(y_test, y_pred),\n",
        "        \"mse\": mean_squared_error(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "def print_metrics(model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Print evaluation metrics for a given model.\n",
        "    \"\"\"\n",
        "    metrics = evaluate(model, X_train, X_test, y_train, y_test)\n",
        "    print(f\"R² Score: {metrics['r2']}\")\n",
        "    print(f\"Root Mean Squared Error: {metrics['mse'] ** 0.5}\")\n",
        "\n",
        "def hyperparameter_tuning(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning using HalvingGridSearchCV.\n",
        "\n",
        "    Args:\n",
        "        X_train, y_train: Training data.\n",
        "\n",
        "    Returns:\n",
        "        best_model: The best model from grid search.\n",
        "    \"\"\"\n",
        "    param_grid = {\n",
        "        \"device\": [\"cpu\"],  # Use GPU if available\n",
        "        \"eta\": np.arange(0.01, 0.51, 0.05),  # Learning rate\n",
        "        \"max_depth\": range(3, 8),  # Tree depth\n",
        "        \"lambda\": range(0, 5)  # Regularization parameter\n",
        "    }\n",
        "\n",
        "    grid_search = HalvingGridSearchCV(\n",
        "        estimator=XGBRegressor(random_state=42),\n",
        "        param_grid=param_grid,\n",
        "        cv=3,\n",
        "        factor=2,\n",
        "        scoring=\"r2\",\n",
        "        verbose=1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "m6BE7dFcsqlZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Prepare Data Dynamically\n",
        "Apply the data preparation function to both `clean_df` and `eng_df` for dynamic feature selection."
      ],
      "metadata": {
        "id": "I5Ije8jkrMx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare Data Dynamically\n",
        "# For clean_df\n",
        "X_train_clean, X_test_clean, y_train_clean, y_test_clean = prepare_data(clean_df, target_column=\"price\")\n",
        "\n",
        "# For eng_df\n",
        "X_train_eng, X_test_eng, y_train_eng, y_test_eng = prepare_data(eng_df, target_column=\"price\")"
      ],
      "metadata": {
        "id": "nRtVAy-5sudT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Train and Evaluate Models\n",
        "Train models on both datasets and evaluate their performance."
      ],
      "metadata": {
        "id": "0ogwVtCyswrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train and Evaluate Models\n",
        "# Train and evaluate on clean_df\n",
        "print(\"Evaluation for clean_df:\")\n",
        "model_clean = XGBRegressor(random_state=42, device=\"cpu\")\n",
        "print_metrics(model_clean, X_train_clean, X_test_clean, y_train_clean, y_test_clean)\n",
        "\n",
        "# Train and evaluate on eng_df\n",
        "print(\"Evaluation for eng_df:\")\n",
        "model_eng = XGBRegressor(random_state=42, device=\"cpu\")\n",
        "print_metrics(model_eng, X_train_eng, X_test_eng, y_train_eng, y_test_eng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xUfjz5ts1sY",
        "outputId": "673814a7-0259-43e7-ca3d-987d827049af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for clean_df:\n",
            "R² Score: 0.6524236320093406\n",
            "Root Mean Squared Error: 79.1567399482531\n",
            "Evaluation for eng_df:\n",
            "R² Score: 0.6662380750111618\n",
            "Root Mean Squared Error: 77.56774659113088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Hyperparameter Tuning\n",
        "Perform hyperparameter tuning for XGBoost on both datasets and evaluate the best models."
      ],
      "metadata": {
        "id": "xx5Cn2Otsy5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Hyperparameter Tuning\n",
        "print(\"Hyperparameter Tuning for clean_df:\")\n",
        "best_model_clean = hyperparameter_tuning(X_train_clean, y_train_clean)\n",
        "print_metrics(best_model_clean, X_train_clean, X_test_clean, y_train_clean, y_test_clean)\n",
        "\n",
        "print(\"Hyperparameter Tuning for eng_df:\")\n",
        "best_model_eng = hyperparameter_tuning(X_train_eng, y_train_eng)\n",
        "print_metrics(best_model_eng, X_train_eng, X_test_eng, y_train_eng, y_test_eng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw2XzGZLszcj",
        "outputId": "9e85eb8b-6065-4c25-c27f-187d4f229c02"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter Tuning for clean_df:\n",
            "n_iterations: 8\n",
            "n_required_iterations: 8\n",
            "n_possible_iterations: 8\n",
            "min_resources_: 357\n",
            "max_resources_: 45702\n",
            "aggressive_elimination: False\n",
            "factor: 2\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 250\n",
            "n_resources: 357\n",
            "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 125\n",
            "n_resources: 714\n",
            "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 1428\n",
            "Fitting 3 folds for each of 63 candidates, totalling 189 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 32\n",
            "n_resources: 2856\n",
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
            "----------\n",
            "iter: 4\n",
            "n_candidates: 16\n",
            "n_resources: 5712\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "----------\n",
            "iter: 5\n",
            "n_candidates: 8\n",
            "n_resources: 11424\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "----------\n",
            "iter: 6\n",
            "n_candidates: 4\n",
            "n_resources: 22848\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "----------\n",
            "iter: 7\n",
            "n_candidates: 2\n",
            "n_resources: 45696\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
            "R² Score: 0.6433817100660428\n",
            "Root Mean Squared Error: 80.1797293221148\n",
            "Hyperparameter Tuning for eng_df:\n",
            "n_iterations: 8\n",
            "n_required_iterations: 8\n",
            "n_possible_iterations: 8\n",
            "min_resources_: 357\n",
            "max_resources_: 45702\n",
            "aggressive_elimination: False\n",
            "factor: 2\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 250\n",
            "n_resources: 357\n",
            "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 125\n",
            "n_resources: 714\n",
            "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 1428\n",
            "Fitting 3 folds for each of 63 candidates, totalling 189 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 32\n",
            "n_resources: 2856\n",
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
            "----------\n",
            "iter: 4\n",
            "n_candidates: 16\n",
            "n_resources: 5712\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "----------\n",
            "iter: 5\n",
            "n_candidates: 8\n",
            "n_resources: 11424\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "----------\n",
            "iter: 6\n",
            "n_candidates: 4\n",
            "n_resources: 22848\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "----------\n",
            "iter: 7\n",
            "n_candidates: 2\n",
            "n_resources: 45696\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
            "R² Score: 0.6390428573824367\n",
            "Root Mean Squared Error: 80.66601434094336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Findings\n",
        "1. Engineered Features Help:\n",
        "  - Incorporating engineered features related to metro station proximity slightly improved the model’s ability to predict Airbnb prices. This supports the hypothesis that geography, specifically proximity to landmarks and metro stations, influences pricing.\n",
        "2. Overfitting Risk:\n",
        "  - The decline in performance after hyperparameter tuning suggests the potential for overfitting or a mismatch in parameter settings for the given data.\n",
        "3. Room for Improvement:\n",
        "  - An R² of ~0.65 indicates that there are other important factors (e.g., seasonal trends, detailed property descriptions, host quality) that the model has not accounted for."
      ],
      "metadata": {
        "id": "LnMi14Ga0cvC"
      }
    }
  ]
}